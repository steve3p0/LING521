{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LING 521: Applied English Grammar\n",
    "### Text Analysis Script #1 \n",
    "TODO: write up an intro\n",
    "\n",
    "### Part 1. Load Dependencies\n",
    "Import the required packages (install them if you haven't already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import string\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "#from jupyter_datatables import init_datatables_mode\n",
    "from IPython.display import display, HTML\n",
    "#init_datatables_mode()\n",
    "\n",
    "#%matplotlib inline\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify software dependencies loaded: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(sys.version)\n",
    "print(\"My library versions:\")\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"ntlk: {nltk.__version__}\")\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "\n",
    "#print(f\"Pandas: {pd.__version__}\")\n",
    "#print(f\"Scipy: {sp.__version__}\")\n",
    "#print(f\"nose: {nose.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Natural Language Processing Tool Kit (NLTK) Package\n",
    "NLTK POS Taggers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('tagsets')\n",
    "# Show definition of tags\n",
    "# tagset_upenn = nltk.help.upenn_tagset()\n",
    "\n",
    "# Function Words: We explored using NLTK stop words, but ultimately we did not use it\n",
    "# We combined ADP, PRON, DET, CONJ into Inserts\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "insert_words = ('yeah', 'Ok', 'ahh')\n",
    "\n",
    "messages = ['Gym?',\n",
    "            'yeah be there in about a half',\n",
    "            'Ok see you when you get here!',\n",
    "            'Seconds away',\n",
    "            'Meet me between smith and cramer asap',\n",
    "            'I got you and Taylor tix in pit section.',\n",
    "            'Get some milk please',\n",
    "            'Chk email',\n",
    "            'Made it',\n",
    "            'Do u know where u saved that movie on my compute',\n",
    "            'Im meeting some dude from the internet for happy hour ahh!',\n",
    "            'Wed is dinner for renetta call us soon',\n",
    "            'where r u???',\n",
    "            'pinball']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tag POS in 2007 Text Messages:\n",
    "Iterate thru each text message in our 2007 Text Message Corpus and tag the PART-OF-SPEECH of each word.\n",
    "\n",
    "<b>NOTE:</b> We could have written this code to simply get the counts on the whole corpus,\n",
    "but for this assignment, message level analysis made it easier to confirm with manual counts.\n",
    "\n",
    "This is how the tagset would look like if we simply used the upenn tagset:<br>\n",
    "Counter({'NOUN': 29, 'VERB': 12, 'ADP': 9, 'ADV': 8, 'PRON': 8, '.': 7, 'DET': 4, 'ADJ': 3, 'CONJ': 2})\n",
    "\n",
    "However, in order to satisfy the requirements, we need to modify our tagset as follows:\n",
    "1. Inserts: Check if a word is in our inserts set\n",
    "2. Function Words: Combine ADP, PRON, DET, CONJ into FUNCTOR\n",
    "3. Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "counter_list = []\n",
    "words = defaultdict(list)\n",
    "\n",
    "for msg in messages:\n",
    "    tokens = nltk.word_tokenize(msg)\n",
    "    word_tag_pairs = nltk.pos_tag(tokens, tagset='universal')\n",
    "    print(f\"\\nRaw Message: {msg}\")\n",
    "    print(f\"Words with POS Tags: {word_tag_pairs}\")\n",
    "\n",
    "    # Build a dictionary of words, grouped by POS\n",
    "    for w, tag in word_tag_pairs:\n",
    "        if w in insert_words:\n",
    "            tag = \"Inserts\"\n",
    "            words[tag].append(w)\n",
    "        elif tag in ('PRON', 'DET', 'ADP', 'CONJ'):\n",
    "            tag = \"FUNCTOR\"\n",
    "            words[tag].append(w)\n",
    "        elif tag != '.':\n",
    "            words[tag].append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum of totals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "counter_pos = {k: len(v) for k,v in words.items()}\n",
    "total_words = sum(counter_pos.values())\n",
    "\n",
    "print(f\"\\nPOS Counts: {counter_pos}\")\n",
    "print(f\"\\nTotal Words: {total_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.  Calculate Percentages and Normed Frequencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List of words by POS\n",
    "print(f\"\\nSummary - POS Tagging\")\n",
    "print(f\"counter_pos: {counter_pos}\")\n",
    "print(f\"Nouns: {words['NOUN']}\")\n",
    "print(f\"Verbs: {words['VERB']}\")\n",
    "print(f\"Adjectives: {words['ADJ']}\")\n",
    "print(f\"Adverbs: {words['ADV']}\")\n",
    "print(f\"Function Words: {words['FUNCTOR']}\")\n",
    "print(f\"Inserts: {words['Inserts']}\")\n",
    "\n",
    "# Gather Counts\n",
    "raw_counts_nouns = counter_pos['NOUN']\n",
    "raw_counts_verbs = counter_pos['VERB']\n",
    "raw_counts_adverbs = counter_pos['ADV']\n",
    "raw_counts_adjectives = counter_pos['ADJ']\n",
    "raw_counts_functors = counter_pos['FUNCTOR']\n",
    "raw_counts_inserts = counter_pos['Inserts']\n",
    "\n",
    "percent_nouns = raw_counts_nouns / total_words\n",
    "percent_verbs = raw_counts_verbs / total_words\n",
    "percent_adjectives = raw_counts_adjectives / total_words\n",
    "percent_adverbs = raw_counts_adverbs / total_words\n",
    "percent_functors = raw_counts_functors / total_words\n",
    "percent_inserts = raw_counts_inserts / total_words\n",
    "\n",
    "norm_counts_nouns = percent_nouns * 1000\n",
    "norm_counts_verbs = percent_verbs * 1000\n",
    "norm_counts_adjectives = percent_adjectives * 1000\n",
    "norm_counts_adverbs = percent_adverbs * 1000\n",
    "norm_counts_functors = percent_functors * 1000\n",
    "norm_counts_inserts = percent_inserts * 1000\n",
    "\n",
    "# Print Counts\n",
    "print(f\"\\nRaw Counts:\")\n",
    "print(f\"Nouns: {raw_counts_nouns}\")\n",
    "print(f\"Verbs: {raw_counts_verbs}\")\n",
    "print(f\"Adjectives: {raw_counts_adjectives}\")\n",
    "print(f\"Adverbs: {raw_counts_adverbs}\")\n",
    "print(f\"Function Words: {raw_counts_functors}\")\n",
    "print(f\"Inserts: {raw_counts_inserts}\")\n",
    "print(f\"Total Words: {total_words}\")\n",
    "\n",
    "print(f\"\\nPercentages:\")\n",
    "print(f\"Nouns: {percent_nouns:.1%}\")\n",
    "print(f\"Verbs: {percent_verbs:.1%}\")\n",
    "print(f\"Adjectives: {percent_adjectives:.1%}\")\n",
    "print(f\"Adverbs: {percent_adverbs:.1%}\")\n",
    "print(f\"Function Words: {percent_functors:.1%}\")\n",
    "print(f\"Inserts: {percent_inserts:.1%}\")\n",
    "total_percentages = sum([percent_nouns, percent_verbs, percent_adjectives, percent_adverbs,\n",
    "                        percent_functors, percent_inserts])\n",
    "print(f\"Total Percentages: {total_percentages:.1%}\")\n",
    "\n",
    "print(f\"\\nNormed Counts Per 1000:\")\n",
    "print(f\"Nouns: {norm_counts_nouns:0.1f}\")\n",
    "print(f\"Verbs: {norm_counts_verbs:0.1f}\")\n",
    "print(f\"Adjectives: {norm_counts_adjectives:0.1f}\")\n",
    "print(f\"Adverbs: {norm_counts_adverbs:0.1f}\")\n",
    "print(f\"Function Words: {norm_counts_functors:0.1f}\")\n",
    "print(f\"Inserts: {norm_counts_inserts:0.1f}\")\n",
    "total_norm_counts = sum([norm_counts_nouns, norm_counts_verbs, norm_counts_adjectives, norm_counts_adverbs,\n",
    "                        norm_counts_functors, norm_counts_inserts])\n",
    "print(f\"Total Norm Counts: {total_norm_counts:0.1f}\")\n",
    "\n",
    "data = {'Word Class': ['Nouns', 'Verbs', 'Adjectives', 'Adverbs', 'Function Words', 'Inserts', 'Totals'],\n",
    "        'Raw Counts': [raw_counts_nouns, raw_counts_verbs, raw_counts_adjectives, raw_counts_adverbs, raw_counts_functors, raw_counts_inserts, total_words],\n",
    "        'Percentages': [percent_nouns, percent_verbs, percent_adjectives, percent_adverbs, percent_functors, percent_inserts, total_percentages ],\n",
    "        'Normed per 1000': [norm_counts_nouns, norm_counts_verbs, norm_counts_adjectives, norm_counts_adverbs, norm_counts_functors, norm_counts_inserts, total_norm_counts]}\n",
    "counts_table = pd.DataFrame(data, columns=['Word Class', 'Raw Counts', 'Percentages', 'Normed per 1000'])\n",
    "counts_table.style.hide_index()\n",
    "# counts_table.style.format(\n",
    "# {\n",
    "#     'var1': \"{:.2f}\",\n",
    "#     'var2': \"{:.2f}\",\n",
    "#     'var3': \"{:.2%}\"\n",
    "# })\n",
    "\n",
    "\n",
    "# output = counts_table.to_html(formatters={\n",
    "#     'Word Class': '{}',\n",
    "#     'Raw Counts': '{}',\n",
    "#     'Percentages': '{:,.2%}'.format,\n",
    "#     'Normed per 1000': '{:,.1f}'.format\n",
    "# })\n",
    "output = counts_table.to_html(formatters={\n",
    "    'Word Class': '{}',\n",
    "    'Raw Counts': '{}',\n",
    "    'Percentages': '{}',\n",
    "    'Normed per 1000': '{}'\n",
    "})\n",
    "\n",
    "display(HTML(output))\n",
    "\n",
    "#display(HTML(output.to_html(index=False)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Plot Stacked Bar Chart\n",
    "Create a stacked bar graph displaying the raw counts, percentages, and normed frequencies per 1000 words for the lexical classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Graph it!\n",
    "width = 0.7\n",
    "p1 = plt.bar(width=width, x=1, height=norm_counts_nouns)\n",
    "p2 = plt.bar(width=width, x=1, height=norm_counts_verbs, bottom=norm_counts_nouns)\n",
    "p3 = plt.bar(width=width, x=1, height=norm_counts_adverbs, bottom=norm_counts_nouns + norm_counts_verbs)\n",
    "p4 = plt.bar(width=width, x=1, height=norm_counts_adjectives, bottom=norm_counts_nouns + norm_counts_verbs + norm_counts_adverbs)\n",
    "\n",
    "plt.ylabel('Normed Counts Per 1000 Words')\n",
    "plt.title('2007 Text Messages: Frequency of Lexical Word Classes')\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "\n",
    "actual_last_value = norm_counts_nouns + norm_counts_verbs + norm_counts_adverbs\n",
    "max_y_value = total_words * 1000\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), handles=(p1[0], p2[0], p3[0], p4[0]), labels=('Nouns', 'Verbs', 'Adverbs', 'Adjectives'))\n",
    "#plt.autoscale(False)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Summary\n",
    "Summary Write-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Footer: used to force updates\n",
    "Last Updated: 11/12/2019 1:27 PM  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}