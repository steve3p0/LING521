{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LING 521: Applied English Grammar\n",
    "### Text Analysis Script #1 \n",
    "\n",
    "### Part 1. Identify POS Tags in 2007 Text Messages:\n",
    "Here are the instructions from the assignment for Part 1: \n",
    "\n",
    "    a. Underline the inserts.  Circle function words.  For lexical words, categorize them as noun, verb, adjective or adverb. \n",
    "\t   (Write N, V, Adj, or Adv above each lexical word.)\n",
    "    b. Count each class (N, V, Adj, Adv, inserts, function words) and record the count.  These are your raw counts.\n",
    "    c. Count every word to get a total word count. Do not just total your counts of classes. \n",
    "       Instead, use this as a way to confirm your counting. Do your counts of each class make this total? \n",
    "       If not, figure out where your mistakes are.\n",
    "             \n",
    "We have created a script that will automatically identify the word classes as instructed above, calculate totals, \n",
    "and create a stacked bar chart for analysis.\n",
    "\n",
    "##### Approach: \n",
    "We Iterate thru each text message in our 2007 Text Message Corpus and tag the PART-OF-SPEECH of each word.\n",
    "\n",
    "<b>NOTE:</b> We could have written this code to simply get the counts on the whole corpus,\n",
    "but for this assignment, message level analysis made it easier to confirm with manual counts.\n",
    "\n",
    "This is how the tagset would look like if we simply used the upenn tagset:<br>\n",
    "Counter({'NOUN': 29, 'VERB': 12, 'ADP': 9, 'ADV': 8, 'PRON': 8, '.': 7, 'DET': 4, 'ADJ': 3, 'CONJ': 2})\n",
    "\n",
    "However, in order to satisfy the requirements, we need to modify our tagset as follows:\n",
    "1. Inserts: Check if a word is in our inserts set, if so set POS tag to INSERT\n",
    "2. Function Words: Combine ADP, PRON, DET, CONJ into FUNCTOR\n",
    "3. Remove Punctuation\n",
    "\n",
    "Alright, let's begin!\n",
    "\n",
    "### Load Dependencies\n",
    "#### Import the required packages (install them if you haven't already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "plt.rcdefaults()\n",
    "#%matplotlib inline\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display my Python Package Library versions and verify software dependencies loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"sys.version: {sys.version}\")\n",
    "print(f\"NLTK: {nltk.__version__}\")\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")\n",
    "\n",
    "#print(f\"Scipy: {sp.__version__}\")\n",
    "#print(f\"nose: {nose.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Natural Language Processing Tool Kit (NLTK) Package\n",
    "Load the following Part-of-Speech (POS) Taggers from the Natural Language Toolkit Library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('punkt')\n",
    "nltk.download('tagsets')\n",
    "# Show definition of tags\n",
    "# tagset_upenn = nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTE ON FUNCTION WORDS:</b><br>\n",
    "With respect to Function Words, we explored using the [NLTK Stopword corpus](https://www.nltk.org/book/ch02.html#stopwords_index_term)\n",
    "when it was revealed to us that the NLTK's Stopword corpus, despite varying by domain, is basically \n",
    "[a manually currated 'general purpose' list of function words](https://stackoverflow.com/questions/41811790/function-vs-content-words#41813460)\n",
    " freely available to use in our tagger.\n",
    "\n",
    "The other method we tried was mapping tags of pars of speech that closely resembled the different types of words that \n",
    "closely resemnbled function words (as defined in the the Longman textbook).  We have listed the POS Tags below that \n",
    "we combined into one 'FUNCTOR' POS Tag.\n",
    " \n",
    "| Longman POS          | NLTK POS       | NLTK POS Tag    | Function Word Tag (New) |\n",
    "| :-------------------:|:--------------:| :--------------:|:-----------------------:|\n",
    "| Prepositions         | Adpositions    | ADP             | FUNCTOR                 |\n",
    "| Pronouns             | Pronouns       | PRON            | FUNCTOR                 |\n",
    "| Determiners          | Determiners    | DET             | FUNCTOR                 |\n",
    "| Coordinators / </br> Subordinators   | Conjunctions | CONJ | FUNCTOR              |\n",
    "| *Auxiliary Verbs     |      -         |       -         |  Not Implemented        |  \n",
    "| *Adverbial Particles |      -         |       -         |  Not Implemented        |  \n",
    "\n",
    "*We did not get have enough time to implement Auxiliary and Adverbial Particle detection.\n",
    "\n",
    "Ultimately we went with Stopwords, because the counts we were getting were closer to what the instructor reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Corpora:\n",
    "\n",
    "##### Insert Words\n",
    "We did not make an exhastive list of insert words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "insert_words = ('yeah', 'Ok', 'ahh', 'please')\n",
    "insert_words = set(w.lower() for w in insert_words)\n",
    "\n",
    "# Use NLTK Stopwords to detect Function Words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "messages = ['Gym?',\n",
    "            'yeah be there in about a half',\n",
    "            'Ok see you when you get here!',\n",
    "            'Seconds away',\n",
    "            'Meet me between smith and cramer asap',\n",
    "            'I got you and Taylor tix in pit section.',\n",
    "            'Get some milk please',\n",
    "            'Chk email',\n",
    "            'Made it',\n",
    "            'Do u know where u saved that movie on my compute',\n",
    "            'Im meeting some dude from the internet for happy hour ahh!',\n",
    "            'Wed is dinner for renetta call us soon',\n",
    "            'where r u???',\n",
    "            'pinball']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag each word with a Part-of-Speech (POS)\n",
    "Iterate thru each text message in our 2007 Text Message Corpus and tag the PART-OF-SPEECH of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "counter_list = []\n",
    "words = defaultdict(list)\n",
    "words_stops = []\n",
    "messages_list_tagged_custom = []\n",
    "\n",
    "for msg in messages:\n",
    "    tokens = nltk.word_tokenize(msg)\n",
    "    #tokens = [w for w in tokens if w not in stopwords_fw] \n",
    "    \n",
    "    msg_tagged = nltk.pos_tag(tokens, tagset='universal')\n",
    "    print(f\"\\nMessage (RAW): {msg}\")\n",
    "    print(f\"Message (NLTK   POS Tagged): {msg_tagged}\")\n",
    "\n",
    "    msg_tagged_custom = []\n",
    "    # Build a dictionary of words, grouped by POS\n",
    "    for w, tag in msg_tagged:\n",
    "        # NOTE: We are just keeping track of stopwords and reporting in addition to the other tags\n",
    "        # words that are tagged as 'STOPS' will also have their regular POS Tag\n",
    "        if w.lower() in stopwords:\n",
    "            words_stops.append(w)\n",
    "            tag = 'FUNCTOR'\n",
    "        elif w.lower() in insert_words:\n",
    "            tag = 'INSERT'\n",
    "        elif tag in ('PRON', 'DET', 'ADP', 'CONJ'):\n",
    "            tag = 'FUNCTOR'\n",
    "        elif tag == '.':\n",
    "            continue\n",
    "\n",
    "        words[tag].append(w)\n",
    "        msg_tagged_custom.append((w, tag))\n",
    "\n",
    "    messages_list_tagged_custom.append(msg_tagged_custom)\n",
    "    print(f\"Message (Custom POS Tagged): {msg_tagged_custom}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum of totals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "counter_pos = {k: len(v) for k,v in words.items()}\n",
    "total_words = sum(counter_pos.values())\n",
    "\n",
    "print(f\"\\nPOS Counts: {counter_pos}\")\n",
    "print(f\"\\nTotal Words: {total_words}\")\n",
    "print(f\"Total NLTK Stopwords: {len(words_stops)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Words grouped by POS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "row_labels = ['Nouns', 'Verbs', 'Adjectives', 'Adverbs', 'Function Words', 'INSERT', '*Stopwords']\n",
    "words_by_pos = [ ', '.join(words['NOUN']), ', '.join(words['VERB']), ', '.join(words['ADJ']), ', '.join(words['ADV']), \n",
    "                 ', '.join(words['FUNCTOR']), ', '.join(words['INSERT']), ', '.join(words_stops) ]   \n",
    "data = {'Word Class': row_labels,\n",
    "        'Words': words_by_pos}\n",
    "df = pd.DataFrame(data, columns=['Word Class', 'Words'])\n",
    "\n",
    "# Set table styles\n",
    "styles = [ dict(selector=\"th\", props=[('text-align', 'center')]),\n",
    "           dict(selector=\"th\", props=[('white-space', 'nowrap')]),\n",
    "           dict(selector=\"td\", props=[('text-align', 'left')]) ]\n",
    "styled_df = (df.style\n",
    "             .set_properties(subset=df.columns[0],  **{'white-space':'nowrap'})\n",
    "             .set_table_styles(styles))\n",
    "\n",
    "html = styled_df.hide_index().render()\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NLTK Stopwords are provided for the instructor to evaluate how well it correlates function words.\n",
    "\n",
    "### Part 2.  Calculate Percentages, Normed Frequencies, and Plot Stacked Bar Chart\n",
    "Here are the instructions from the assignment for Part 2: \n",
    "\n",
    "    a. Calculate percentages.  What percentage of the total words are lexical words? Function words?  Inserts?  \n",
    "    b. Calculate normed frequencies per 1000 words for the lexical classes.  Make a quick figure like Figure 2.1. \n",
    "       This is a stacked bar.\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Gather Counts\n",
    "raw_counts = counter_pos\n",
    "percentages = {pos: wc / total_words for pos, wc in raw_counts.items()}\n",
    "norm_counts = {pos: wc / total_words * 1000 for pos, wc in raw_counts.items()}\n",
    "\n",
    "row_labels = ['Nouns', 'Verbs', 'Adjectives', 'Adverbs', 'Function Words', 'INSERT']\n",
    "raw_counts_list = [ counter_pos['NOUN'], counter_pos['VERB'], counter_pos['ADJ'], counter_pos['ADV'], \n",
    "                    counter_pos['FUNCTOR'], counter_pos['INSERT'], ]\n",
    "\n",
    "percentages_list = [ percentages['NOUN'], percentages['VERB'], percentages['ADJ'], percentages['ADV'], \n",
    "                     percentages['FUNCTOR'], percentages['INSERT'], ]\n",
    "\n",
    "norm_counts_list = [ norm_counts['NOUN'], norm_counts['VERB'], norm_counts['ADJ'], norm_counts['ADV'], \n",
    "                     norm_counts['FUNCTOR'], norm_counts['INSERT'], ]\n",
    "\n",
    "data = {'Word Class': row_labels,\n",
    "        'Raw Counts': raw_counts_list,\n",
    "        'Percentages':percentages_list,\n",
    "        'Normed per 1000': norm_counts_list }\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Word Class', 'Raw Counts', 'Percentages', 'Normed per 1000'])\n",
    "df.loc[df.index.max()+1]=['Total']+df.sum().tolist()[1:]\n",
    "\n",
    "# Construct a mask of which columns are numeric\n",
    "numeric_col_mask = df.dtypes.apply(lambda d: issubclass(np.dtype(d).type, np.number))\n",
    "# Set table styles\n",
    "styles = [ dict(selector=\"th\", props=[('text-align', 'center')]) ]\n",
    "\n",
    "styled_df = (df.style\n",
    "                # Format the percentage and Normed per 1000 fields\n",
    "                .format({'Percentages': \"{:.1%}\",  'Normed per 1000': \"{:,.1f}\"})\n",
    "                # right-align the numeric columns and set their width\n",
    "                .set_properties(subset=df.columns[numeric_col_mask],  **{'text-align':'right'})\n",
    "                # left-align the non-numeric columns and set their width\n",
    "                .set_properties(subset=df.columns[~numeric_col_mask], **{'text-align':'left'})\n",
    "                # center the header\n",
    "                .set_table_styles(styles) \n",
    "            )\n",
    "\n",
    "html = styled_df.hide_index().render()\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Stacked Bar Chart\n",
    "Create a stacked bar graph displaying the raw counts, percentages, and normed frequencies per 1000 words for the lexical classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Graph it!\n",
    "width = 0.7\n",
    "\n",
    "raw_counts = counter_pos\n",
    "percentages = {pos: wc / total_words for pos, wc in raw_counts.items()}\n",
    "norm_counts = {pos: pct / total_words for pos, pct in percentages.items()}\n",
    "\n",
    "p1 = plt.bar(width=width, x=1, height=norm_counts['NOUN'])\n",
    "p2 = plt.bar(width=width, x=1, height=norm_counts['VERB'], bottom=norm_counts['NOUN'])\n",
    "p3 = plt.bar(width=width, x=1, height=norm_counts['ADV'], bottom=norm_counts['NOUN'] + norm_counts['VERB'])\n",
    "p4 = plt.bar(width=width, x=1, height=norm_counts['ADJ'], bottom=norm_counts['NOUN'] + norm_counts['VERB'] + norm_counts['ADV'])\n",
    "\n",
    "plt.ylabel('Normed Counts Per 1000 Words')\n",
    "plt.title('2007 Text Messages: Frequency of Lexical Word Classes')\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "\n",
    "max_y_value = total_words * 1000\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), handles=(p1[0], p2[0], p3[0], p4[0]), labels=('Nouns', 'Verbs', 'Adverbs', 'Adjectives'))\n",
    "#plt.autoscale(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part 3.  Summary\n",
    "Answer the research questions.  Writing out the answer is your Text Analysis 1 assignment.\n",
    "\n",
    "The research questions (slightly reworded from the class handout – use these):\n",
    "1.  **If this sample is typical, did text messages in 2007 consist almost exclusively of lexical words,\n",
    "     as was many people’s intuition?**\n",
    "\n",
    "    No.  Out of 75 words, 41% of them were function words. I had assumed with a T9 (Text on 9 Keys) numeric keypad,\n",
    "    there would have been a lot less function words. The counts suggest otherwise.\n",
    "\n",
    "\n",
    "2.  **How do the proportions of the lexical word classes in the text messages compare to the proportions in the\n",
    "    registers in Figure 2.1 of the SG (p. 23)?**\n",
    "    \n",
    "    *That is, would you say text messages are like one of those registers in their lexical word class use?  Explain.*\n",
    "\n",
    "    In order to make such a comparison, we need to compare just the proportions of the lexical word classes to in \n",
    "    our stacked bar chart to the Figure 2.1.  My assumption is that the text messages would be more like the \n",
    "    conversational register than any of the others, since text messaging is a form of conversation.  However, the\n",
    "    word counts that this script produces does not really match any of the registers that we see in Figure 2.1.\n",
    "    \n",
    "    If we compare the instructor counts that were provided in class with Figure 2.1, we can see that it closely \n",
    "    resembles the conversational register.  This descrepancy, between script counts and the instructor counts, \n",
    "    would be a good topic for further discussion.\n",
    "\n",
    "**Bonus for fun:**\n",
    "**How do you think the characteristics of text messages in 2017 would differ from these in 2007,\n",
    "especially in terms of the proportions of lexical word classes?**\n",
    "\n",
    "    Interesting question.  On the one hand, typing is much easier in 2017, so one might think that there would\n",
    "    be less abbreviated shorthand for individual words and therefore less ambiguity.  There would be probably\n",
    "    more acronymns, but I think that any good updated parser would be able to handle that.\n",
    "    Also, You would have a lot more emojis. Nouns would be easy to normalize to text, but I'd have to think\n",
    "    more about whether other parts of speech, like verbs would have emoji use.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}